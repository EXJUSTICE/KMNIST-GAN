{"cells":[{"metadata":{"_uuid":"447d9500d3cb0b8e094325d2dfe2084f1769d983"},"cell_type":"markdown","source":"This kernel use GAN(Generative Adversarial Network) to generate different ancient Japanese Hiragana, using KMNIST images.\nBased on implementation \"GAN with MLP on MNIST\" by Vincent Kao, itself based on the reference by Erik Linderen.\n\nFirst step is to initalize and import the images"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from keras.datasets import mnist\nfrom keras.layers import Input, Dense, Reshape, Flatten, Lambda\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nimport keras.backend as K\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom PIL import Image\n\n#For this project, we will only be using train_images\n#To further improve the accuracy of the GAN, you could involve labels\nPATH=\"../input/\"\ntrain_images = np.load(PATH+'kmnist-train-imgs.npz')['arr_0']\ntest_images = np.load(PATH+'kmnist-test-imgs.npz')['arr_0']\ntrain_labels = np.load(PATH+'kmnist-train-labels.npz')['arr_0']\ntest_labels = np.load(PATH+'kmnist-test-labels.npz')['arr_0']\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Set up network parameters\nThese will be handy later, also do some sampling parameters"},{"metadata":{"trusted":true,"_uuid":"3c8e774dc7afd6030f9ee4d75332e616504dba41"},"cell_type":"code","source":"img_rows = 28\nimg_cols = 28\nchannels = 1\nimg_shape = (img_rows, img_cols, channels)\nlatent_dim = 10\nbatch_size = 16\nepsilon_std = 1.0\n\n# View the dataset to get an idea of what we're dealing with\ndef plot_sample_images_data(images, labels):\n    plt.figure(figsize=(12,12))\n    for i in range(10):\n        imgs = images[np.where(labels == i)]\n        lbls = labels[np.where(labels == i)]\n        for j in range(10):\n            plt.subplot(10,10,i*10+j+1)\n            plt.xticks([])\n            plt.yticks([])\n            plt.grid(False)\n            plt.imshow(imgs[j], cmap=plt.cm.binary)\n            plt.xlabel(lbls[j])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff04f1d7ec2a95a9010bf19fd911a60f0aa45c63","scrolled":true},"cell_type":"code","source":"plot_sample_images_data(train_images, train_labels)\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std)\n    return z_mean + K.exp(z_log_var / 2) * epsilon","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec29f27c389819931df9640cd441b8dd9831befb"},"cell_type":"markdown","source":"## Define a function to build an encoder\nEncoders job is to take an existing image and reduce it to its most simplest form?"},{"metadata":{"trusted":true,"_uuid":"b64ff4dd32694422a89f8468368baed10d1e2ab7"},"cell_type":"code","source":"def build_encoder():\n    img = Input(shape=img_shape)\n    h = Flatten()(img)\n    h = Dense(512)(h)\n    h = LeakyReLU(alpha=0.2)(h)\n    h = Dense(512)(h)\n    h = LeakyReLU(alpha=0.2)(h)\n    mu = Dense(latent_dim)(h)\n    log_var = Dense(latent_dim)(h)\n    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([mu, log_var])\n    return Model(img, z)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10ded117cd9054188ccda9b8f206f1faf92ab360"},"cell_type":"markdown","source":"## Define a function to build an decoder\nThe decoders job is to build the image from an encoding, which is an artificial representation"},{"metadata":{"trusted":true,"_uuid":"03ad6f3f1ad517fadf3a21c419d51fc848965843"},"cell_type":"code","source":"def build_decoder():\n    model = Sequential()\n    model.add(Dense(512, input_dim=latent_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    # tanh is more robust: gradient not equal to 0 around 0\n    model.add(Dense(np.prod(img_shape), activation='tanh'))\n    model.add(Reshape(img_shape))\n    model.summary()\n    z = Input(shape=(latent_dim,))\n    img = model(z)\n    return Model(z, img)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e35b534855cecde50b6887c1738f8de44e6cdd77"},"cell_type":"markdown","source":"## Define a function to build an discriminator\nThe discriminator's job is to judge the generated images for authenticity, whether it's real or fake"},{"metadata":{"trusted":true,"_uuid":"166a4a06102de7c2974a638d47ddc6d7c2455d34"},"cell_type":"code","source":"def build_discriminator():\n    #Added 1024 layer in discrim\n    model = Sequential()\n    model.add(Dense(1024, input_dim=latent_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(256))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    model.summary()\n    encoded_repr = Input(shape=(latent_dim,))\n    validity = model(encoded_repr)\n    return Model(encoded_repr, validity)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfa11eaf3929ab1112ad6c62027ab6d0f66bcf29"},"cell_type":"markdown","source":"## Build GAN"},{"metadata":{"trusted":true,"_uuid":"fbf254a682c25a1c93477f151a4240276480c0d8"},"cell_type":"code","source":"optimizer = Adam(0.0002, 0.5)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='binary_crossentropy',\n                      optimizer=optimizer,\n                      metrics=['accuracy'])\n\n# Build the encoder / decoder\nencoder = build_encoder()\ndecoder = build_decoder()\n\nimg = Input(shape=img_shape)\n# The generator takes the image, encodes it and reconstructs it\n# from the encoding\nencoded_repr = encoder(img)\nreconstructed_img = decoder(encoded_repr)\n\n# For the adversarial_autoencoder model we will only train the generator\n# if discriminator is attached to generator, set this flag to fix discriminator\ndiscriminator.trainable = False\n\n# The discriminator determines validity of the encoding\nvalidity = discriminator(encoded_repr)\n\n# The adversarial_autoencoder model  (stacked generator and discriminator)\n#We define the loss as MSE and binary_crossentropy\nadversarial_autoencoder = Model(img, [reconstructed_img, validity])\nadversarial_autoencoder.compile(loss=['mse', 'binary_crossentropy'], loss_weights=[0.999, 0.001], optimizer=optimizer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a9b82f2fb6046392f0ae5af0cf35f1f646568ad"},"cell_type":"markdown","source":"## Define a function to train GAN"},{"metadata":{"trusted":true,"_uuid":"2b69e5ccbb97fb5004207c1434565dc8bf14ff4f"},"cell_type":"code","source":"def train(epochs, batch_size=128, sample_interval=50):\n    # Load the dataset\n    X_train =train_images \n\n    # Normalization: Rescale -1 to 1\n    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n    X_train = np.expand_dims(X_train, axis=3)\n\n    # Adversarial ground truths\n    valid = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n        #  Train Discriminator\n\n        # Select a random batch of images\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        imgs = X_train[idx]\n\n        latent_fake = encoder.predict(imgs)\n        latent_real = np.random.normal(size=(batch_size, latent_dim))\n\n        # Train the discriminator\n        # let latent_real's output is close to 1\n        d_loss_real = discriminator.train_on_batch(latent_real, valid)\n        # let latent_fake's output is close to 0\n        d_loss_fake = discriminator.train_on_batch(latent_fake, fake)\n        d_loss = 0.7 * np.add(d_loss_real, d_loss_fake)\n\n        # Train the generator\n        # decrease reconstruction error and let discriminator's output is close to 1\n        g_loss = adversarial_autoencoder.train_on_batch(imgs, [imgs, valid])\n\n        # If at save interval\n        if epoch % sample_interval == 0:\n            # Plot the progress\n            print(\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (\n                epoch, d_loss[0], 100 * d_loss[1], g_loss[0], g_loss[1]))\n            # save generated image samples\n            sample_images(epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f87a7fb0da952f5182ed84611b345db235768271"},"cell_type":"code","source":"# Save generated images per specified epochs \ndef sample_images(epoch):\n    r, c = 5, 5\n    z = np.random.normal(size=(r * c, latent_dim))\n    gen_imgs = decoder.predict(z)\n    gen_imgs = 0.5 * gen_imgs + 0.5\n    fig, axs = plt.subplots(r, c)\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap=plt.cm.binary)\n            axs[i, j].axis('off')\n            cnt += 1\n    fig.savefig(\"mnist_%d.png\" % epoch)\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2442206b2e4c5dfc52984030899a1d52e7fccc04"},"cell_type":"markdown","source":"## Train GAN\nAs we've set the discrimnator to be not trainable, we are only training the generator"},{"metadata":{"trusted":true,"_uuid":"61a157ac77476498d501fb1a83250c0f10fcb763"},"cell_type":"code","source":"epochs = 90000\nsample_interval = 2000\nsample_count = epochs/sample_interval","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1668132e775f90654543867e6e9358d41875332d"},"cell_type":"code","source":"train(epochs=epochs, batch_size=batch_size, sample_interval=sample_interval)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27ef4606f80e46f8e0471ab3dbf4ae9a8de0ce4d"},"cell_type":"markdown","source":"## Show generated MNIST images per 200 epochs"},{"metadata":{"trusted":true,"_uuid":"b48b2f21165e00f13a020bc190cc9ba0203e253b"},"cell_type":"code","source":"Image.open('mnist_6000.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f9a160f1b42840319177cf4d03936f359816fadb"},"cell_type":"code","source":"Image.open('mnist_10000.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc0393f28833e7c651c2ee9f902fcedc791350c3","collapsed":true},"cell_type":"code","source":"Image.open('mnist_16000.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe951ba525bdb3342cac553145dfa45ce67b154f","collapsed":true},"cell_type":"code","source":"Image.open('mnist_20000.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02b63cd182c23f0579f42601fe711f4e382746de"},"cell_type":"code","source":"Image.open('mnist_28000.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"828045b687cdadff2b90afc89b2e05b05f095b07"},"cell_type":"markdown","source":"## Show single generated image"},{"metadata":{"trusted":true,"_uuid":"84090fd817eea9db0da9e511d6b18317c7ceec69"},"cell_type":"code","source":"z = np.random.normal(size=(1, latent_dim))\ngen_imgs = decoder.predict(z)\ngen_imgs = 0.5 * gen_imgs + 0.5\nplt.imshow(gen_imgs[0, :, :, 0], cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f1a7716ae716bb5c5e716aceee738eceabbf53c"},"cell_type":"markdown","source":"## Reference\n[Keras - Adversarial Autoencoder(AAE)](https://github.com/eriklindernoren/Keras-GAN#adversarial-autoencoder)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cc68e032fca585ed9b0ffe64ec6a1db7099dcc90"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}